# 神经网络的简单步骤

## 模型分类

根据输入的数据集是连续的实数值，还是离散的标签，可以分为**回归任务**和**分类任务**。

## 模型结构三要素

模型假设

评价函数

优化算法

## 数据归一化处理

对每个特征进行归一化处理，使得每个特征的取值缩放到0~1之间。这样做有两个好处：一是模型训练更高效；二是特征前的权重大小可以代表该变量对预测结果的贡献度（因为每个特征值本身的范围相同）。



## 模型设计

模型设计是深度学习模型关键要素之一，也称为网络结构设计，相当于模型的假设空间，即实现模型“前向计算”（从输入到输出）的过程。

前向网络：个人理解，就是从输入到输出

例如：`z = wx + b`,对于该线性分类模型，w-样本权重，b-偏差

## 训练配置

模型设计完成后，需要通过训练配置寻找模型的最优值，即通过损失函数来衡量模型的好坏。训练配置也是深度学习模型关键要素之一。

通过损失函数（loss）来衡量模型的优劣：计算损失函数值需要把每个样本的损失函数值都考虑到。

通过神经网络完成预测值和损失函数的计算。

## 训练过程

如何求解参数`w`和`b`的数值，这个过程也称为模型训练过程。

训练过程是深度学习模型的关键要素之一，其目标是让定义的损失函数LossLossLoss尽可能的小，也就是说找到一个参数解www和bbb，使得损失函数取得极小值。

### 梯度下降法求解损失函数

在现实中存在大量的函数正向求解容易，但反向求解较难，被称为单向函数，这种函数在密码学中有大量的应用。密码锁的特点是可以迅速判断一个密钥是否是正确的(已知x，求y很容易)，但是即使获取到密码锁系统，无法破解出正确的密钥是什么（已知y，求x很难）。

这种情况特别类似于一位想从山峰走到坡谷的盲人，他看不见坡谷在哪（无法逆向求解出Loss导数为0时的参数值），但可以伸脚探索身边的坡度（当前点的导数值，也称为梯度）。那么，求解Loss函数最小值可以这样实现：从当前的参数取值，一步步的按照下坡的方向下降，直到走到最低点。这种方法笔者称它为“盲人下坡法”。哦不，有个更正式的说法“梯度下降法”。

### 随机梯度下降法（ Stochastic Gradient Descent）

在上述程序中，每次损失函数和梯度计算都是基于数据集中的全量数据。对于波士顿房价预测任务数据集而言，样本数比较少，只有404个。但在实际问题中，数据集往往非常大，如果每次都使用全量数据进行计算，效率非常低，通俗地说就是“杀鸡焉用牛刀”。由于参数每次只沿着梯度反方向更新一点点，因此方向并不需要那么精确。一个合理的解决方案是每次从总的数据集中随机抽取出小部分数据来代表整体，基于这部分数据计算梯度和损失来更新参数，这种方法被称作随机梯度下降法（Stochastic Gradient Descent，`SGD`），核心概念如下：

- mini-batch：每次迭代时抽取出来的一批数据被称为一个mini-batch。
- batch_size：一个mini-batch所包含的样本数目称为batch_size。
- epoch：当程序迭代的时候，按mini-batch逐渐抽取出样本，当把整个数据集都遍历到了的时候，则完成了一轮训练，也叫一个epoch。启动训练时，可以将训练的轮数`num_epochs`和`batch_size`作为参数传入。